{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc56cd2d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div style=\"padding: 60px;\n",
    "  text-align: center;\n",
    "  background: #d4afb9;\n",
    "  color: #003049;\n",
    "  font-size: 20px;\">\n",
    "  <h2>Kafka and Python for Real-Time Data Processing</h2>\n",
    "   <hr>\n",
    "</div>\n",
    "\n",
    "\n",
    "- **Develop By** : Dwi Gustin Nurdialit\n",
    "- **Last Updated**: February 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf1f5aa",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Training Objectives](#toc1_1_)    \n",
    "  - [🌟 Building a Kafka Producer in Python](#toc1_2_)    \n",
    "      - [🌈 Apa Itu Kafka Producer?](#toc1_2_1_1_)    \n",
    "      - [🙂 Mengapa Kafka Producer Penting?](#toc1_2_1_2_)    \n",
    "      - [🛠️ Cara Kerja Kafka Producer](#toc1_2_1_3_)    \n",
    "    - [1️⃣ Create Kafka Producer using `confluent_kafka`](#toc1_2_2_)    \n",
    "      - [🌐 Menjalankan Kafka](#toc1_2_2_1_)    \n",
    "      - [🛠️ Configuring a Kafka Producer](#toc1_2_2_2_)    \n",
    "    - [2️⃣ Fetching Latest BTC Price](#toc1_2_3_)    \n",
    "      - [📃 1. Download Data](#toc1_2_3_1_)    \n",
    "      - [🌐 2. Filter Data](#toc1_2_3_2_)    \n",
    "      - [📊 3. Ambil Data Terakhir/Terbaru](#toc1_2_3_3_)    \n",
    "      - [⏳ 4. Konversi Format Waktu](#toc1_2_3_4_)    \n",
    "      - [🔐 5. Konversi ke Format JSON](#toc1_2_3_5_)    \n",
    "    - [3️⃣ Send Data to Kafka](#toc1_2_4_)    \n",
    "      - [🎲 1. Encode Data JSON](#toc1_2_4_1_)    \n",
    "      - [🛠️ 3. Kirim Ke Kafka Producer](#toc1_2_4_2_)    \n",
    "    - [💪 Summary](#toc1_2_5_)    \n",
    "  - [🎧 Building a Kafka Consumer in Python](#toc1_3_)    \n",
    "      - [🔍 Kafka Consumer](#toc1_3_1_1_)    \n",
    "      - [🤖 Understanding Kafka Consumer](#toc1_3_1_2_)    \n",
    "      - [🔎 Role of a Consumer in Kafka](#toc1_3_1_3_)    \n",
    "      - [📊 How Consumers Read Data from Kafka](#toc1_3_1_4_)    \n",
    "    - [1️⃣ Server and Port Configuration](#toc1_3_2_)    \n",
    "      - [🛠️ Configuring a Kafka Consumer](#toc1_3_2_1_)    \n",
    "    - [2️⃣ Fetch Data from Kafka](#toc1_3_3_)    \n",
    "      - [🔄 Polling Data dari Kafka](#toc1_3_3_1_)    \n",
    "    - [3️⃣ Process and Store Data](#toc1_3_4_)    \n",
    "      - [📝 Memproses Data yang Diterima](#toc1_3_4_1_)    \n",
    "  - [🖥️ Display Real-time Data](#toc1_4_)    \n",
    "    - [`try`, `except`, dan `finally`](#toc1_4_1_)    \n",
    "      - [**1️⃣ `try`**](#toc1_4_1_1_)    \n",
    "      - [**2️⃣ `except`**](#toc1_4_1_2_)    \n",
    "      - [**3️⃣ `finally`**](#toc1_4_1_3_)    \n",
    "  - [📈 Display Real-time Visualization](#toc1_5_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Training Objectives](#toc0_)\n",
    "\n",
    "\n",
    "\n",
    "- **Kafka Overview**  \n",
    "  - What is Kafka?  \n",
    "  - How Kafka Works in Real-Time Data Processing  \n",
    "\n",
    "- **Kafka Components**  \n",
    "  - Broker  \n",
    "  - Topics and Partitions  \n",
    "  - Producers and Consumers  \n",
    "  - Consumer Groups  \n",
    "  - Offset Management  \n",
    "  - Zookeeper (Optional for Older Versions)  \n",
    "\n",
    "- **Setting Up Kafka**\n",
    "  - Running Kafka Locally using Terminal\n",
    "\n",
    "- **Kafka Producer**  \n",
    "  - Understanding Kafka Producer\n",
    "  - Role of a Producer in Kafka  \n",
    "  - How Data is Sent to Kafka  \n",
    "\n",
    "- **Configuring a Kafka Producer**  \n",
    "  - Server and Port Configuration  \n",
    "  - Topics and Message Structure (Key-Value Pair)  \n",
    "  - Serialization and Deserialization  \n",
    "  - Acknowledgments (acks)  \n",
    "\n",
    "- **Building a Kafka Producer in Python**  \n",
    "  - Writing a Simple Producer Application  \n",
    "\n",
    "- **Kafka Consumer**  \n",
    "  - Understanding Kafka Consumer\n",
    "  - Role of a Consumer in Kafka  \n",
    "  - How Consumers Read Data from Kafka  \n",
    "\n",
    "- **Configuring a Kafka Consumer**  \n",
    "  - Server and Port Configuration  \n",
    "  - Consumer Groups and Group ID  \n",
    "  - Offset Management (latest vs. earliest)  \n",
    "  - Auto Commit and Manual Commit  \n",
    "\n",
    "- **Building a Kafka Consumer in Python**  \n",
    "  - Writing a Simple Consumer Application  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc1fac6",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-info\"><br>\n",
    "  <center><h2>Building a Kafka Producer in Python</h2></center>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746dfe33",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[🌟 Building a Kafka Producer in Python](#toc0_)\n",
    "\n",
    "Apache Kafka adalah platform streaming data yang populer digunakan untuk memproses data secara real-time. Salah satu komponen penting dalam Kafka adalah **Producer**, yang bertugas mengirim data ke Kafka untuk kemudian dikonsumsi oleh **Consumer**. Pada modul ini, kita akan membangun Kafka Producer menggunakan Python dan mengirimkan data harga Bitcoin secara real-time! 📊💰\n",
    "\n",
    "---\n",
    "\n",
    "#### <a id='toc1_2_1_1_'></a>[🌈 Apa Itu Kafka Producer?](#toc0_)\n",
    "\n",
    "Kafka Producer adalah komponen yang bertugas mengirimkan pesan ke **Kafka Topics**. Producer mengambil data dari sumber tertentu (misalnya API, database, atau sensor IoT) dan mengirimkannya ke Kafka agar dapat dikonsumsi oleh **Kafka Consumers**.\n",
    "\n",
    "#### <a id='toc1_2_1_2_'></a>[🙂 Mengapa Kafka Producer Penting?](#toc0_)\n",
    "Kafka Producer memiliki beberapa fitur utama:\n",
    "- **Asynchronous Messaging**: Pesan dikirim secara asinkron untuk meningkatkan performa.\n",
    "- **Message Batching**: Producer dapat mengirimkan beberapa pesan dalam satu batch untuk efisiensi.\n",
    "- **Serialization**: Data dikonversi ke format yang dapat diproses oleh Kafka, seperti JSON atau Avro.\n",
    "- **Acknowledgment (acks)**: Producer dapat mengonfirmasi apakah pesan telah diterima oleh Kafka.\n",
    "\n",
    "➡️ Sekarang, mari kita lihat bagaimana Kafka Producer bekerja dalam sistem streaming data! 🌟\n",
    "\n",
    "---\n",
    "\n",
    "#### <a id='toc1_2_1_3_'></a>[🛠️ Cara Kerja Kafka Producer](#toc0_)\n",
    "\n",
    "Ketika Kafka Producer mengirimkan data ke Kafka, ada beberapa proses yang terjadi:\n",
    "1. **Mengambil data** dari sumber eksternal seperti API atau database.\n",
    "2. **Memproses atau memformat ulang data** agar sesuai dengan kebutuhan.\n",
    "3. **Mengirim data ke Kafka Topics**, tempat penyimpanan sementara sebelum dikonsumsi oleh Consumer.\n",
    "4. **Menyebarkan data ke berbagai partisi** dalam Kafka untuk load balancing dan parallel processing.\n",
    "\n",
    "> 📍 Lalu, bagaimana cara membuat Kafka Producer di Python? Mari kita mulai dari konfigurasi dasarnya!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_2_'></a>[1️⃣ Create Kafka Producer using `confluent_kafka`](#toc0_)\n",
    "\n",
    "Untuk membuat Kafka Producer di Python, kita akan menggunakan pustaka `confluent_kafka` untuk mengirimkan harga Bitcoin terbaru ke Kafka. Sebelum mengirimkan data, kita perlu mengatur konfigurasi Producer terlebih dahulu.\n",
    "\n",
    "Sebelum memulai, pastikan bahwa:\n",
    "\n",
    "- Kafka sudah terinstal di sistem.\n",
    "- Memiliki pustaka Python `confluent_kafka` dan `yfinance` yang telah terinstal (`pip install confluent_kafka yfinance`).\n",
    "- Kafka sedang berjalan di mesin lokal Anda.\n",
    "\n",
    "#### <a id='toc1_2_2_1_'></a>[🌐 Menjalankan Kafka](#toc0_)\n",
    "\n",
    "Sebelum menjalankan Kafka Producer, pastikan bahwa Kafka sudah berjalan di mesin kita. Jika Kafka tidak berjalan, Producer tidak akan dapat mengirim data. Kita bisa menjalankan Kafka menggunakan Docker dengan perintah berikut:\n",
    "\n",
    "```sh\n",
    "docker run -p 2181:2181 -p 9092:9092 -e ADVERTISED_HOST=localhost 0bda86cD\n",
    "```\n",
    "\n",
    "#### <a id='toc1_2_2_2_'></a>[🛠️ Configuring a Kafka Producer](#toc0_)\n",
    "\n",
    "**🛠️ 1. Server and Port Configuration**\n",
    "\n",
    "Kafka Producer harus mengetahui **alamat dan port** dari Kafka Cluster agar dapat mengirim pesan. Konfigurasi ini dilakukan melalui `bootstrap.servers`, yang menentukan alamat broker Kafka yang akan digunakan untuk mengirimkan data.\n",
    "\n",
    "Agar dapat mengirim data ke Kafka, kita harus membuat Kafka Producer dengan konfigurasi berikut:\n",
    "\n",
    "- `bootstrap.servers`: Menentukan alamat broker Kafka yang akan digunakan.\n",
    "- `client.id`: ID unik untuk mengenali producer ini.\n",
    "\n",
    "Mari kita buat konfigurasi Kafka Producer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2120a560",
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e617d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konfigurasi Kafka producer\n",
    "KAFKA_CONFIG ="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6872971a",
   "metadata": {},
   "source": [
    "Selanjutnya, kita membuat instance `Producer` dengan konfigurasi yang telah ditentukan sebelumnya. Fungsi `Producer()` dari pustaka `confluent_kafka` digunakan untuk menginisialisasi producer Kafka yang akan mengelola koneksi dengan broker Kafka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c651e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "producer = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4177465",
   "metadata": {},
   "source": [
    "**📝 2. Topics and Message Structure (Key-Value Pair)**\n",
    "\n",
    "Kafka menyimpan pesan dalam **Topics**, yang berfungsi seperti \"folder\" tempat data dikelompokkan. Setiap pesan yang dikirimkan ke Kafka memiliki format **Key-Value Pair**, yang memungkinkan Kafka melakukan partisi data berdasarkan **Key** tertentu.\n",
    "\n",
    "Mari kita tentukan topik Kafka serta interval pengiriman data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b007d7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC = \n",
    "INTERVAL = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6093a62c",
   "metadata": {},
   "source": [
    "> ➡️ Setelah kita menentukan **topic**, langkah selanjutnya adalah menyiapkan **struktur pesan** yang akan dikirimkan ke Kafka. Dalam kasus ini, kita akan mengirimkan harga Bitcoin secara berkala. Oleh karena itu, kita perlu mengambil data harga Bitcoin terbaru terlebih dahulu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_3_'></a>[2️⃣ Fetching Latest BTC Price](#toc0_)\n",
    "\n",
    "Untuk mendapatkan data harga Bitcoin terbaru, kita akan menggunakan pustaka `yfinance`. Langkah-langkahnya adalah sebagai berikut:\n",
    "\n",
    "#### <a id='toc1_2_3_1_'></a>[📃 1. Download Data](#toc0_)\n",
    "\n",
    "Kita akan mengambil harga Bitcoin dari Yahoo Finance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388382bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea121143",
   "metadata": {},
   "source": [
    "\n",
    "Kita akan menggunakan fungsi `yf.download()` untuk mengambil data harga Bitcoin. Beberapa parameter yang perlu diperhatikan:\n",
    "\n",
    "- `tickers=[\"BTC-USD\"]`: Mengambil harga Bitcoin dalam USD.\n",
    "- `period=\"1d\"`: Mengambil data selama 1 hari terakhir.\n",
    "- `interval=\"1m\"`: Mengambil data tiap 1 menit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc8626c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_2_3_2_'></a>[🌐 2. Filter Data](#toc0_)\n",
    "Data yang diperoleh dari Yahoo Finance berisi banyak kolom. Kita akan menggunakan metode `xs` untuk memilih hanya data **Bitcoin**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3940507",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_2_3_3_'></a>[📊 3. Ambil Data Terakhir/Terbaru](#toc0_)\n",
    "\n",
    "Kita hanya memerlukan kolom **Datetime** dan **Close Price**. Untuk itu, kita akan men-subset data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c556b6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_data = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_2_3_4_'></a>[⏳ 4. Konversi Format Waktu](#toc0_)\n",
    "\n",
    "Kafka lebih mudah memproses data dalam format **YYYY-MM-DD HH:MM:SS**. Kita akan menggunakan `strftime()` untuk melakukan konversi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ba6489",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_data['Datetime'] = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_2_3_5_'></a>[🔐 5. Konversi ke Format JSON](#toc0_)\n",
    "\n",
    "\n",
    "Kafka hanya bisa menerima data dalam format **string JSON**, sehingga kita harus mengonversi data terlebih dahulu. Untuk itu, kita akan menggunakan metode `to_dict()` dengan parameter `orient=\"records\"`, yang akan mengonversi DataFrame ke dalam format JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7509a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = latest_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fda18d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54857439",
   "metadata": {},
   "source": [
    "> ➡️ Setelah mendapatkan data terbaru, kita akan mengirimnya ke Kafka menggunakan Producer! 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fbcbe2",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_4_'></a>[3️⃣ Send Data to Kafka](#toc0_)\n",
    "\n",
    "Setelah mendapatkan harga Bitcoin terbaru dalam format JSON, kita akan mengirimkannya ke Kafka Topic menggunakan Kafka Producer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_2_4_1_'></a>[🎲 1. Encode Data JSON](#toc0_)\n",
    "\n",
    "Kafka hanya menerima data dalam format **byte string**, sehingga kita perlu mengonversinya terlebih dahulu dengan tahapan:\n",
    "\n",
    "1. **Mengonversi dictionary ke JSON** (`json.dumps(data)`).\n",
    "2. **Mengonversi ke format byte** (`encode('utf-8')`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dda0ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b350a53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### <a id='toc1_2_4_2_'></a>[🛠️ 3. Kirim Ke Kafka Producer](#toc0_)\n",
    "Setelah data siap, kita dapat mengirimkan pesan ke Kafka menggunakan \n",
    "\n",
    "- `produce(TOPIC, message)`: Mengirimkan pesan ke topik Kafka.\n",
    "- `flush()`: Memastikan semua data terkirim sebelum program selesai dieksekusi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f6edce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_5_'></a>[💪 Summary](#toc0_)\n",
    "\n",
    "Di bagian ini, kita telah belajar:\n",
    "\n",
    "- ✅ Menggunakan `confluent_kafka` untuk membuat Kafka Producer di Python.\n",
    "- ✅ Mengonfigurasi Kafka Producer dengan parameter yang tepat.\n",
    "- ✅ Mengambil data harga Bitcoin secara real-time menggunakan `yfinance`.\n",
    "- ✅ Mengirimkan data terbaru ke Kafka Topic.\n",
    "\n",
    "Selanjutnya, kita akan membuat **Kafka Consumer** untuk membaca data yang telah dikirimkan! Stay tuned! 🔥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530d5587",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-info\"><br>\n",
    "  <center><h2>Building a Kafka Consumer in Python</h2></center>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50cb7b0",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[🎧 Building a Kafka Consumer in Python](#toc0_)\n",
    "\n",
    "#### <a id='toc1_3_1_1_'></a>[🔍 Kafka Consumer](#toc0_)\n",
    "Kafka Consumer adalah komponen dalam Apache Kafka yang bertugas membaca dan memproses data dari topik Kafka. Berbeda dengan Producer yang mengirim data, Consumer bertindak sebagai penerima dan mengelola pesan secara efisien.\n",
    "\n",
    "#### <a id='toc1_3_1_2_'></a>[🤖 Understanding Kafka Consumer](#toc0_)\n",
    "Consumer membaca data dari Kafka dalam bentuk batch atau streaming. Dalam sistem yang kompleks, Consumer sering bekerja dalam grup untuk memastikan skalabilitas dan reliabilitas pemrosesan data.\n",
    "\n",
    "#### <a id='toc1_3_1_3_'></a>[🔎 Role of a Consumer in Kafka](#toc0_)\n",
    "Consumer bertugas mengambil pesan dari topik Kafka, memprosesnya, dan menyimpannya ke sistem lain seperti database atau data warehouse. Consumer juga bertanggung jawab mengatur offset untuk memastikan tidak ada data yang hilang atau diproses dua kali.\n",
    "\n",
    "#### <a id='toc1_3_1_4_'></a>[📊 How Consumers Read Data from Kafka](#toc0_)\n",
    "Consumer membaca data dari Kafka berdasarkan offset tertentu. Offset ini bisa diatur sebagai `earliest` (membaca dari awal) atau `latest` (membaca hanya pesan baru).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_2_'></a>[1️⃣ Server and Port Configuration](#toc0_)\n",
    "\n",
    "#### <a id='toc1_3_2_1_'></a>[🛠️ Configuring a Kafka Consumer](#toc0_)\n",
    "Sebelum membuat Consumer, kita perlu mengonfigurasikan beberapa parameter penting:\n",
    "\n",
    "- **`bootstrap.servers`** → Menentukan alamat dan port broker Kafka yang digunakan untuk menerima data.\n",
    "- **`group.id`** → Mengelompokkan beberapa Consumer agar bisa membaca data secara bersamaan tanpa duplikasi.\n",
    "- **`auto.offset.reset`** → Mengontrol dari mana Consumer mulai membaca pesan:\n",
    "  - `earliest` → Membaca dari awal jika belum ada offset yang disimpan.\n",
    "  - `latest` → Membaca hanya pesan terbaru.\n",
    "- **`enable.auto.commit`** → Jika `True`, Kafka akan otomatis menyimpan posisi offset. Jika `False`, kita perlu melakukan commit offset secara manual untuk lebih banyak kontrol.\n",
    "\n",
    "Berikut konfigurasi dasarnya:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbcb477",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from confluent_kafka import Consumer, KafkaException, KafkaError\n",
    "\n",
    "pd.options.display.float_format = '{:,.3f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f735313",
   "metadata": {},
   "source": [
    "**1. Konfigurasi Kafka Consumer**  \n",
    "Sebelum dapat membaca data dari Kafka, kita perlu mengatur beberapa konfigurasi dasar untuk **Kafka Consumer**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46168531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konfigurasi Kafka Consumer\n",
    "consumer_config = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7507a280",
   "metadata": {},
   "source": [
    "**2. Membuat Kafka Consumer**  \n",
    "Setelah mengatur konfigurasi, kita perlu membuat objek **Consumer** yang akan digunakan untuk membaca data dari Kafka.  \n",
    "\n",
    "- Menggunakan `Consumer()` menerima parameter berupa konfigurasi yang sudah kita definisikan sebelumnya (`consumer_config`).  \n",
    "- Buat variabel `consumer` yang akan digunakan untuk menerima data dari Kafka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83b963c",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c008f78",
   "metadata": {},
   "source": [
    "**3. Subscribe ke Topik Kafka**  \n",
    "Untuk mengakses data dari Kafka, kita perlu menentukan **topik** mana yang akan dibaca.  \n",
    "\n",
    "- **`topic_name`** → Nama topik yang ingin dibaca, dalam hal ini `\"bitcoin-topic\"`.  \n",
    "- **`consumer.subscribe([topic_name])`** → Memberitahu Kafka bahwa consumer ini ingin membaca pesan dari topik yang sudah ditentukan.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2ae8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subscribe ke topik Kafka\n",
    "topic_name = \n",
    "consumer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f7ba65",
   "metadata": {},
   "source": [
    "**Additional:**  \n",
    "\n",
    "- Jika terdapat beberapa topik yang ingin dibaca, kita bisa memberikan daftar topik, misalnya:  \n",
    "  ```python\n",
    "  consumer.subscribe([\"topic1\", \"topic2\"])\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### <a id='toc1_3_3_'></a>[2️⃣ Fetch Data from Kafka](#toc0_)\n",
    "\n",
    "#### <a id='toc1_3_3_1_'></a>[🔄 Polling Data dari Kafka](#toc0_)\n",
    "Kafka Consumer menggunakan metode **`poll(timeout)`** untuk menunggu dan mengambil data dari Kafka. Parameter `timeout` menentukan berapa lama Consumer akan menunggu pesan sebelum mencoba lagi.\n",
    "\n",
    "Berikut cara mengambil data dari Kafka:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c327feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554ffaa9",
   "metadata": {},
   "source": [
    "- **`poll(timeout=1.0)`** → Memerintahkan consumer untuk mengambil pesan dari Kafka dengan batas waktu **1 detik**.  \n",
    "- Jika ada pesan yang tersedia dalam antrean Kafka, `poll()` akan segera mengembalikan pesan tersebut.  \n",
    "- Jika tidak ada pesan dalam waktu yang ditentukan, `poll()` akan mengembalikan `None`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea862bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00488990",
   "metadata": {},
   "source": [
    "- Kafka berhasil mengembalikan sebuah **objek pesan** (`Message`) yang berisi data dari Kafka.  \n",
    "- **`cimpl.Message`** adalah objek yang merepresentasikan pesan Kafka dalam library `confluent-kafka`.  \n",
    "- `0x12e1c0cc0` adalah alamat memori tempat objek `Message` ini disimpan (akan berbeda setiap kali dijalankan).  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1681165",
   "metadata": {},
   "source": [
    "Jika kita ingin melihat isi pesan yang sebenarnya, kita bisa mentranform atau decode pesan dari *bytes* ke *string*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50789d22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### <a id='toc1_3_4_'></a>[3️⃣ Process and Store Data](#toc0_)\n",
    "\n",
    "#### <a id='toc1_3_4_1_'></a>[📝 Memproses Data yang Diterima](#toc0_)\n",
    "Setelah menerima pesan dari Kafka, kita perlu mengonversinya ke dalam format yang mudah digunakan. Di sini, kita akan menggunakan **`json.loads()`** untuk mengubah JSON menjadi dictionary Python.\n",
    "\n",
    "Setelah itu, kita mengubah kolom `Datetime` menjadi format `datetime` agar lebih mudah dianalisis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6c2579",
   "metadata": {},
   "source": [
    "**1. Menyimpan Data dalam List**  \n",
    "Sebelum memproses data, kita akan menyiapkan sebuah list kosong untuk menyimpan data Bitcoin yang diterima dari Kafka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5a6241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data untuk menyimpan harga Bitcoin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0def4639",
   "metadata": {},
   "source": [
    "List `btc_data` akan digunakan untuk menyimpan data harga Bitcoin dalam bentuk pasangan waktu (`timestamp`) dan harga penutupan (`close_price`).\n",
    "\n",
    "---\n",
    "\n",
    "**2. Parsing Data JSON dari Kafka**  \n",
    "Ketika kita menerima pesan dari Kafka, pesan tersebut biasanya dalam format JSON. Oleh karena itu, kita perlu mengonversi pesan tersebut ke dalam bentuk dictionary Python menggunakan `json.loads()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af027229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d5ec65",
   "metadata": {},
   "source": [
    "- **`json.loads()`**: Mengonversi string JSON menjadi dictionary Python yang bisa kita akses lebih lanjut.\n",
    "\n",
    "---\n",
    "\n",
    "**3. Mengonversi Kolom `Datetime` ke Format `datetime`**  \n",
    "Setelah parsing data, kita perlu mengonversi kolom `Datetime` menjadi format `datetime` agar dapat digunakan dalam analisis lebih lanjut, seperti untuk pemrosesan waktu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f609a651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd69fc3",
   "metadata": {},
   "source": [
    "**4. Menambahkan Data ke dalam List**  \n",
    "Setelah kita mendapatkan data yang sudah diproses, kita akan menambahkannya ke dalam list `btc_data` untuk menyimpan setiap titik data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce6775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menambahkan data ke dalam list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd73ed8d",
   "metadata": {},
   "source": [
    "---\n",
    "**5. Mengubah List Menjadi DataFrame untuk Analisis Lebih Lanjut**  \n",
    "Setelah beberapa data terkumpul, kita bisa mengubah list tersebut menjadi sebuah **DataFrame** menggunakan `pandas`, yang memudahkan analisis lebih lanjut dan visualisasi data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64319ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengubah list menjadi DataFrame untuk analisis lebih lanjut\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615c950b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d736ef88",
   "metadata": {},
   "source": [
    "> Pada awalnya, kita mungkin hanya akan melihat satu baris data saja, karena ini adalah data pertama yang diproses dari Kafka."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1f3107",
   "metadata": {},
   "source": [
    "## <a id='toc1_4_'></a>[🖥️ Display Real-time Data](#toc0_)\n",
    "\n",
    "Karena kita ingin memantau data secara real-time, kita perlu memperbarui DataFrame secara terus-menerus dengan polling data dari Kafka. Dalam hal ini, kita akan menggunakan **loop yang berjalan terus-menerus** dan memperbarui tampilan di Jupyter Notebook dengan pembaruan data secara real-time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5039dbf3",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_1_'></a>[`try`, `except`, dan `finally`](#toc0_)\n",
    "\n",
    "Blok **`try-except-finally`** digunakan untuk menangani error atau pengecualian (exception) yang mungkin terjadi saat menjalankan kode. Struktur ini membantu program untuk tetap berjalan meskipun ada error dan untuk memastikan kode tertentu dieksekusi, terlepas dari apakah error terjadi atau tidak.\n",
    "\n",
    "**Struktur General `try`, `except`, dan `finally`**\n",
    "\n",
    "- **`try`**: Di dalam blok ini, kita menulis kode yang mungkin menyebabkan error. Jika error terjadi, eksekusi kode akan berpindah ke blok **`except`**.\n",
    "- **`except`**: Blok ini menangani error yang terjadi dalam blok **`try`**. Kita dapat menentukan jenis error tertentu atau menangani semua jenis error. Jika ada error di blok **`try`**, maka kode di dalam **`except`** yang sesuai dengan error tersebut akan dijalankan.\n",
    "- **`finally`**: Blok ini dijalankan **terlepas dari apakah ada error atau tidak**. Biasanya digunakan untuk menutup file, melepaskan resource, atau menutup koneksi (seperti yang dilakukan dengan `consumer.close()` di contoh kita).\n",
    "\n",
    "\n",
    "```python\n",
    "try:\n",
    "    # Diisi dengan kode yang mungkin menyebabkan error\n",
    "    pass  # Misalnya, membuka file atau melakukan koneksi database\n",
    "    \n",
    "except ExceptionType:\n",
    "    # Diisi dengan kode untuk menangani error\n",
    "    pass  # Menangani error tertentu (misalnya, FileNotFoundError)\n",
    "    \n",
    "finally:\n",
    "    # Diisi dengan kode yang ingin dijalankan terlepas dari apakah ada error atau tidak\n",
    "    pass  # Menutup koneksi atau membersihkan resource\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d1197d",
   "metadata": {},
   "source": [
    "#### <a id='toc1_4_1_1_'></a>[**1️⃣ `try`**](#toc0_)\n",
    "\n",
    "Di dalam blok **`try`**, kita akan memasukkan beberapa kode yang bertanggung jawab untuk menjalankan polling data dari Kafka dan memprosesnya sesuai dengan kebutuhan. Pada kasus ini, kita menambahkan **loop polling**, **pemeriksaan error**, **parsing data**, dan **update tampilan real-time**.\n",
    "\n",
    "Secara umum, kode di dalam blok **`try`** akan terus dieksekusi selama tidak ada error yang terjadi. Jika ada error, maka eksekusi akan berpindah ke blok **`except`**.\n",
    "\n",
    "---\n",
    "\n",
    "Berikut adalah rincian kode yang akan dimasukkan ke dalam **`try`**:\n",
    "\n",
    "- **`while True:`** - loop yang terus berulang dan mengambil data dari Kafka secara terus-menerus.\n",
    "- **`msg = consumer.poll(timeout=1.0)`** - Fungsi polling untuk menunggu pesan dari Kafka. Jika tidak ada pesan dalam waktu 1 detik, proses ini akan mengulang.\n",
    "- `if`:\n",
    "  - **`if msg is None:`** - Jika tidak ada pesan yang diterima, program akan melanjutkan untuk mengecek pesan berikutnya.\n",
    "  - **`if msg.error():`** - Mengecek apakah ada error pada pesan. Jika error terkait dengan akhir partisi (_EOF), maka loop akan terus berjalan. Jika ada error lain, pesan error dicetak dan proses berhenti.\n",
    "- Pemrosesan data:\n",
    "  - **`btc_data_point = json.loads(msg.value().decode('utf-8'))`** - Mengonversi data yang diterima (yang berupa JSON) menjadi format dictionary Python.\n",
    "  - **`btc_data_point['Datetime'] = pd.to_datetime(btc_data_point['Datetime'])`** - Mengubah format waktu menjadi objek `datetime` agar lebih mudah dianalisis.\n",
    "  - **`btc_data.append([btc_data_point['Datetime'], btc_data_point['Close']])`** - Menambahkan data Bitcoin yang baru ke dalam list `btc_data`.\n",
    "  - **`df_btc = pd.DataFrame(btc_data, columns=[\"Time\", \"Price\"])`** - Membuat DataFrame dari data yang telah diterima dan disimpan dalam `btc_data`.\n",
    "- Kebutuhan display:\n",
    "  - **`clear_output(wait=True)`** - Membersihkan output sebelumnya di Jupyter Notebook, agar tampilan data selalu terbarui tanpa membebani tampilan.\n",
    "  - **`display(df_btc)`** - Menampilkan DataFrame yang berisi data Bitcoin yang diterima secara real-time.\n",
    "  - **`time.sleep(0.5)`** - Memberikan jeda setengah detik sebelum memproses data berikutnya, agar tidak membebani notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_4_1_2_'></a>[**2️⃣ `except`**](#toc0_)\n",
    "\n",
    "Pada bagian **`except`**, kita akan menangani error atau kejadian tertentu yang terjadi selama eksekusi kode dalam blok **`try`**. Dalam hal ini, kita akan menangani error **`KeyboardInterrupt`**, yang biasanya terjadi ketika pengguna menekan tombol **Ctrl + C** untuk menghentikan proses. Logika yang digunakan adalah, ketika tombol **stop** ditekan, maka program akan berhenti secara aman dan menampilkan pesan ***\"Stopped by user\"***.\n",
    "\n",
    "\n",
    "```python\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopped by user\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### <a id='toc1_4_1_3_'></a>[**3️⃣ `finally`**](#toc0_)\n",
    "\n",
    "Pada bagian **`finally`**, kita akan menambahkan kode yang harus dieksekusi terlepas dari apakah terjadi error atau tidak dalam blok **`try`** dan **`except`**. Pada kasus ini, kita akan menutup Kafka Consumer dengan cara memanggil **`consumer.close()`**, untuk memastikan koneksi dengan Kafka ditutup dengan aman saat program dihentikan.\n",
    "\n",
    "Contoh kode:\n",
    "\n",
    "```python\n",
    "finally:\n",
    "    consumer.close()  # Menutup Kafka Consumer dengan aman\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09fb36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from confluent_kafka import Consumer, KafkaException, KafkaError\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "\n",
    "pd.options.display.float_format = '{:,.3f}'.format\n",
    "\n",
    "# Konfigurasi Kafka Consumer\n",
    "\n",
    "\n",
    "# Subscribe ke topik Kafka\n",
    "\n",
    "\n",
    "# List untuk menyimpan data Bitcoin\n",
    "\n",
    "\n",
    "# try\n",
    "try:\n",
    "    while True:\n",
    "        msg = consumer.poll(timeout=1.0)  # Polling pesan dari Kafka\n",
    "        \n",
    "        if msg is None:\n",
    "            continue  # Jika tidak ada pesan, lanjutkan loop\n",
    "        \n",
    "        if msg.error():\n",
    "            if msg.error().code() == KafkaError._PARTITION_EOF:\n",
    "                continue  # Jika mencapai akhir partisi, lanjutkan loop\n",
    "            else:\n",
    "                print(f\"Kafka Error: {msg.error()}\")\n",
    "                break\n",
    "        \n",
    "        # Parsing data JSON dari Kafka\n",
    "\n",
    "        \n",
    "        # Menambahkan data ke dalam list\n",
    "\n",
    "        \n",
    "        # Membuat DataFrame\n",
    "\n",
    "        \n",
    "        # Menampilkan data di Jupyter Notebook dengan pembaruan real-time\n",
    "\n",
    "\n",
    "        # Tambahkan jeda agar tidak membebani Jupyter Notebook\n",
    "\n",
    "\n",
    "# except\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopped by user\")\n",
    "\n",
    "# finally\n",
    "finally:\n",
    "    consumer.close()  # Menutup Kafka Consumer dengan aman\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kafka",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
